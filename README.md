# Quantization-Aware-Training-for-LLM-Pruning

This project explores the integration of two model compression strategies—pruning and quantization—to reduce the size and computational load of large language models (LLMs) without significantly compromising accuracy. By embedding quantization-aware training (QAT) into the pruning process, the model is trained to adapt to low-precision arithmetic during compression itself. This synergy enables more effective deployment of LLMs on limited-resource hardware such as edge devices or low-VRAM GPUs. The project highlights a balanced trade-off between efficiency and performance, offering a scalable solution for cost-sensitive AI applications.

