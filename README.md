ğŸš€ Quantization-Aware Training for LLM Pruning
This project explores an integrated approach to compressing Large Language Models (LLMs) using Quantization-Aware Training (QAT) combined with model pruning. The primary goal is to enable efficient deployment of LLMs on resource-constrained devices without significantly sacrificing performance.

ğŸ” Project Summary
Traditional model compression techniques like quantization and pruning are often applied independently. This project combines them during training, allowing the model to:

Adapt to low-precision weights

Undergo structural sparsity

Maintain accuracy on downstream tasks

Benefits:

ğŸš€ Reduced memory and computation cost

âš¡ Lower latency and energy consumption

âœ… Minimal accuracy degradation

This is especially effective for deploying transformer-based LLMs on edge devices or low-VRAM GPUs.

ğŸ“˜ What This Notebook Covers
âœ… Loading and preparing transformer models using ğŸ¤— Hugging Face

âœ‚ï¸ Implementing structured pruning (removing redundant neurons or heads)

ğŸ§  Integrating Quantization-Aware Training via PyTorch

ğŸ“Š Evaluating performance trade-offs: accuracy vs. compression

ğŸ“ˆ Visualizing model sparsity and accuracy trends

ğŸ“Œ Key Techniques Used
Component	Description
Pruning	Structured/unstructured removal of unnecessary weights
Quantization-Aware Training	Simulates low-precision inference (e.g., INT8) during training
Transformer Models	BERT, DistilBERT, and other Hugging Face-based models
PyTorch	For model training, pruning, and quantization APIs
Matplotlib / Seaborn	Visualization of sparsity and performance

ğŸ§ª Results
âœ… Achieved 50â€“80% model sparsity

ğŸ“‰ Enabled INT8-ready inference with quantized weights

âš–ï¸ Maintained performance on downstream NLP tasks with minimal accuracy loss

ğŸ›  Tools & Libraries
ğŸ Python 3.x

ğŸ”¦ PyTorch

ğŸ¤— Hugging Face Transformers

ğŸ–¼ TorchVision

ğŸ“Š NumPy, Matplotlib, Seaborn
