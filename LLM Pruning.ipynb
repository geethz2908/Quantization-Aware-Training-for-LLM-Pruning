{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "***Project_Title :- Quantization-Aware Training for LLM Pruning***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T16:36:44.345475Z",
     "iopub.status.busy": "2024-11-21T16:36:44.344623Z",
     "iopub.status.idle": "2024-11-21T16:36:44.349183Z",
     "shell.execute_reply": "2024-11-21T16:36:44.348278Z",
     "shell.execute_reply.started": "2024-11-21T16:36:44.345440Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The QuantizationLayer class implements a dynamic quantization mechanism that adjusts the precision of neurons based on their energy levels. Neurons with energy above a specified threshold are assigned a higher precision (high_precision), while those below the threshold are assigned a lower precision (base_precision). During the forward pass, each neuron's precision is determined dynamically, and its activations are quantized using a scaling factor derived from the assigned precision. This selective quantization ensures that critical neurons retain finer-grained representations, while less active neurons are compressed, optimizing both accuracy and resource utilization.\n",
    "\n",
    "This aligns with the broader concepts of Adaptive Precision Heterogeneity (APH) and Thermal-Analog Quantization (TAQ). APH dynamically assigns precision across layers and operations based on their performance sensitivity, with critical components like attention layers receiving higher precision and less sensitive layers using lower precision. TAQ, inspired by thermal states, extends this idea to neurons, assigning precision based on their \"energy\" states. High-energy neurons critical for specific tasks retain higher precision, while low-energy neurons are quantized more coarsely, mimicking energy efficiency principles from physics. Together, these concepts enhance model efficiency while preserving critical task performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T16:36:47.186101Z",
     "iopub.status.busy": "2024-11-21T16:36:47.185756Z",
     "iopub.status.idle": "2024-11-21T16:36:47.192053Z",
     "shell.execute_reply": "2024-11-21T16:36:47.191126Z",
     "shell.execute_reply.started": "2024-11-21T16:36:47.186047Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class QuantizationLayer(nn.Module):\n",
    "    def __init__(self, base_precision, high_precision, threshold):\n",
    "        super(QuantizationLayer, self).__init__()\n",
    "        self.base_precision = base_precision\n",
    "        self.high_precision = high_precision\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def quantize(self, x, precision):\n",
    "        scale_factor = 2 ** (precision - 1)\n",
    "        return torch.round(x * scale_factor) / scale_factor\n",
    "\n",
    "    def forward(self, x, neuron_energy):\n",
    "        quantized_output = torch.zeros_like(x)\n",
    "        for i in range(x.size(1)):\n",
    "            precision = self.high_precision if neuron_energy[i] > self.threshold else self.base_precision\n",
    "            quantized_output[:, i] = self.quantize(x[:, i], precision)\n",
    "        return quantized_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the logic used by us in the PruningLayer is based on an energy threshold mechanism, where neurons with low energy values are deactivated to optimize model efficiency. Neuron energy, typically derived from gradient-based metrics such as the mean absolute gradient, is compared against a predefined pruning threshold. Neurons with energy below this threshold are pruned by setting their corresponding values in the input tensor to zero, effectively deactivating them while preserving the tensor's structure. This dynamic pruning process selectively retains only the most significant neurons, reducing computational overhead and potentially enhancing generalization by removing redundant or less impactful neurons during runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T16:36:49.559331Z",
     "iopub.status.busy": "2024-11-21T16:36:49.558778Z",
     "iopub.status.idle": "2024-11-21T16:36:49.564986Z",
     "shell.execute_reply": "2024-11-21T16:36:49.564089Z",
     "shell.execute_reply.started": "2024-11-21T16:36:49.559297Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PruningLayer(nn.Module):\n",
    "    def __init__(self, pruning_threshold):\n",
    "        super(PruningLayer, self).__init__()\n",
    "        self.pruning_threshold = pruning_threshold\n",
    "\n",
    "    def forward(self, x, neuron_energy):\n",
    "        \"\"\"\n",
    "        Prunes neurons based on energy: if the neuron energy is below the threshold, set the neuron to zero.\n",
    "        \"\"\"\n",
    "        # Prune neurons (set to zero) based on neuron energy\n",
    "        pruned_output = x.clone()\n",
    "        for i in range(x.size(1)):\n",
    "            if neuron_energy[i] < self.pruning_threshold:\n",
    "                pruned_output[:, i] = 0\n",
    "        return pruned_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HEGQ combines layer-level adaptability with neuron-specific precision scaling. First, Adaptive Precision Heterogeneity (APH) is applied to set a base precision level for each layer according to its role and sensitivity. Then, within each layer, Thermal-Analog Quantization (TAQ) adjusts the precision further based on the \"energy\" or activation level of individual neurons during specific tasks. This approach creates a multi-tiered precision framework that maximizes efficiency by allocating computational resources dynamically, optimizing for both critical and non-critical layers and neurons.\n",
    "HEGQ leverages both coarse-grained (layer) and fine-grained (neuron) quantization, ensuring that high-sensitivity layers and neurons retain high precision, while less critical areas use lower precision, conserving resources without compromising performance\n",
    "\n",
    "The HEGQModel employs a structured pruning methodology focused on energy-based pruning to enhance computational efficiency while preserving model performance. The model evaluates neuron importance dynamically using gradient-based energy calculations, where neuron energy is determined as the mean absolute value of gradients across samples. Neurons with energy values below a configurable pruning threshold are pruned by setting their activations to zero, effectively removing them from subsequent computations. ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T16:36:53.689510Z",
     "iopub.status.busy": "2024-11-21T16:36:53.688865Z",
     "iopub.status.idle": "2024-11-21T16:36:53.699040Z",
     "shell.execute_reply": "2024-11-21T16:36:53.698209Z",
     "shell.execute_reply.started": "2024-11-21T16:36:53.689476Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class HEGQModel(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, base_precision=4, high_precision=8, pruning_threshold=0.1):\n",
    "        super(HEGQModel, self).__init__()\n",
    "        self.base_model = AutoModel.from_pretrained(model_name)\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Number of layers in the transformer model\n",
    "        num_layers = self.base_model.config.num_hidden_layers\n",
    "\n",
    "        # Dynamic precision levels and thresholds for quantization\n",
    "        layer_precisions = [min(base_precision + i, high_precision) for i in range(num_layers)]\n",
    "        neuron_energy_thresholds = [0.2 + (0.6 * i / (num_layers - 1)) for i in range(num_layers)]\n",
    "\n",
    "        # Quantization and Pruning layers\n",
    "        self.quant_layers = nn.ModuleList([\n",
    "            QuantizationLayer(\n",
    "                base_precision=base_precision,\n",
    "                high_precision=layer_precisions[i],\n",
    "                threshold=neuron_energy_thresholds[i]\n",
    "            )\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.pruning_layer = PruningLayer(pruning_threshold)\n",
    "\n",
    "        # Classification layer\n",
    "        self.classifier = nn.Linear(self.base_model.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        # Forward pass through the base model\n",
    "        outputs = self.base_model(**x)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "        # Calculate classification logits\n",
    "        logits = self.classifier(last_hidden_state[:, 0, :])\n",
    "\n",
    "        # Compute gradients through the classification loss if labels are provided\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "            # Calculate gradients manually\n",
    "            grads = torch.autograd.grad(loss, last_hidden_state, retain_graph=True)[0]\n",
    "\n",
    "            # Quantize based on neuron energy and prune neurons\n",
    "            for i, quant_layer in enumerate(self.quant_layers):\n",
    "                quantized_output = torch.zeros_like(last_hidden_state)\n",
    "                for batch_idx in range(last_hidden_state.size(0)):\n",
    "                    neuron_energy = grads[batch_idx].abs().mean(dim=0)\n",
    "                    # Apply quantization\n",
    "                    quantized_output[batch_idx] = quant_layer(last_hidden_state[batch_idx], neuron_energy)\n",
    "                last_hidden_state = quantized_output\n",
    "\n",
    "            # Apply pruning\n",
    "            pruned_output = self.pruning_layer(last_hidden_state, neuron_energy)\n",
    "            last_hidden_state = pruned_output\n",
    "\n",
    "            return loss, logits\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code demonstrates a classification setup using the HEGQModel applied to the Falcon-7B model. It begins by initializing the tokenizer and setting up the HEGQModel with 10 output classes. If the tokenizer lacks a padding token, the end-of-sequence token is assigned as the padding token to ensure compatibility. The input text is tokenized into a format suitable for model input, with padding and truncation enabled to maintain consistency across different input lengths. A sample label (1) is provided to simulate a classification task. The tokenized inputs and label are passed through the HEGQModel in a forward pass, where the model computes the classification loss and outputs logits for the input text. Finally, the computed loss and logits are printed to evaluate the model's performance for the given input. This workflow highlights the model's ability to handle text classification tasks dynamically using quantization and pruning strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T16:36:56.977741Z",
     "iopub.status.busy": "2024-11-21T16:36:56.977416Z",
     "iopub.status.idle": "2024-11-21T16:39:21.476284Z",
     "shell.execute_reply": "2024-11-21T16:39:21.475344Z",
     "shell.execute_reply.started": "2024-11-21T16:36:56.977711Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f74613a0a2424552a8f32e0d492a09c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ee3fd6ee5964076b3ad7825e352193a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.73M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b93baf6d70cb422a9f56fdc77d4371be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/281 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a44fdfd60e24c9894479acf482ac173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f01f6dcc6d9e47cc96a55311a20db835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/17.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b7a9eb9b7e343dc8a26e1f4c7fb475e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da8320b28b1945f0aa516c19da6eedb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3d3528800b4fffb1865d5249c5b826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.48G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e998319929c547b08e9e9754ccc25d6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.196079730987549\n",
      "Logits: tensor([[ 1.0465,  1.5487,  1.4816,  3.1135, -0.9330, -1.4105,  1.9201, -2.2978,\n",
      "         -1.4990, -2.7751]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"tiiuae/falcon-7b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = HEGQModel(model_name, num_classes=10)\n",
    "\n",
    "# Assign a padding token if it's not already set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Prepare tokenized input\n",
    "input_text = \"Sample text for classification.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Test with tokenized input and label\n",
    "labels = torch.tensor([1])  # Example label for classification\n",
    "\n",
    "# Run forward pass\n",
    "loss, logits = model(inputs, labels=labels)\n",
    "print(\"Loss:\", loss.item())\n",
    "print(\"Logits:\", logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30788,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
